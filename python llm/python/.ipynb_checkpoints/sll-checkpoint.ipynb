{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "260978fc-d107-4afa-953e-e2bdc90f3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27eadfab-72b6-42f1-87f7-c3baf2979f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "CPU times: user 5.46 ms, sys: 17.7 ms, total: 23.2 ms\n",
      "Wall time: 40.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c8d8ac5-c7ad-4799-83a4-554425a2351b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139773\n"
     ]
    }
   ],
   "source": [
    "with open(\"shakespear.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "print(len(text))\n",
    "vocab_size = len(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9267ca66-f74a-423f-a924-b0b820f9de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename, nmbr_of_samples, lenght_of_samples, for_training, len_for_test):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "    text2 = []\n",
    "    sorted_set = sorted(set(text))\n",
    "    dictionary_set = dict()\n",
    "    for i in range(len(sorted_set)):\n",
    "        dictionary_set[sorted_set[i]] = i\n",
    "    #print(sorted_set)\n",
    "    #print(dictionary_set)\n",
    "    for i in range(len(text)):\n",
    "        break\n",
    "    x = np.zeros((nmbr_of_samples, lenght_of_samples), dtype=float)\n",
    "    y = np.zeros((nmbr_of_samples, 1))\n",
    "    if (for_training == 1):\n",
    "        minimum = 0\n",
    "        maximum = len(text) - len_for_test - 1 - lenght_of_samples\n",
    "    else:\n",
    "        minimum = len(text) - len_for_test\n",
    "        maximum = len(text) - 1 - lenght_of_samples\n",
    "    #print(\"set: \", sorted(set(text)))\n",
    "    for i in range(nmbr_of_samples):\n",
    "        r = random.randint(minimum, maximum)\n",
    "        string = text[r:r+lenght_of_samples+1]\n",
    "        string2 = [0, 0, 0, 0, 0, 0]\n",
    "        #string = [ord(char) for char in string]\n",
    "        for ii in range(len(string)):\n",
    "            string2[ii] = dictionary_set[string[ii]]\n",
    "        x[i, :] = string2[0:5]\n",
    "        y[i, :] = string2[5]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ccb5075-f145-4a7d-b928-54d2b91187fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class slm(nn.Module):\\n    def __init__(self, vocab_size):\\n        super().__init__()\\n        self.token_embeding_table = nn.Embedding(vocab_size, vocab_size)\\n    def forward(self, index, targets):\\n        logits = self.token_embeding_table(index)\\n\\n        \\n        if targets is None:\\n            loss = None\\n        else:\\n            B, T, C = logits.shape\\n            logits = logits.view(B*T, C)\\n            targets = targets.view(B*T)\\n            loss = F.cross_entropy(logits, targets)\\n            \\n        return logits\\n        \\n    def generate(self, index, max_new_tokens):\\n        for _ in range(max_new_tokens):\\n            logits, loss = self.forward(index)\\n            logits = logits[:, -1, :]\\n            probs = F.softmax(logits, dim=-1)\\n            index_next = torch.multinomial(probs, num_samples-1)\\n            index = torch.cat((index, index_next), dim=-1)\\n        return index'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class slm(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embeding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    def forward(self, index, targets):\n",
    "        logits = self.token_embeding_table(index)\n",
    "\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits\n",
    "        \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(index)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            index_next = torch.multinomial(probs, num_samples-1)\n",
    "            index = torch.cat((index, index_next), dim=-1)\n",
    "        return index\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b5371e8-bf83-4c51-ba72-39f799ad8996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, input_size, learning_rate, num_epochs):\n",
    "        self.model = SimpleModel(input_size)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def train(self, inputs, targets):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # Forward pass\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Print progress\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{self.num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        with torch.no_grad():\n",
    "            return self.model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c5226e2-c3c7-4b1b-b9ee-cd8d0f56da43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.33 ms, sys: 0 ns, total: 3.33 ms\n",
      "Wall time: 2.68 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_size = 10\n",
    "x, y = get_data(\"shakespear.txt\", 10, input_size, False, 10000)\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "#print(x)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eb751b-a227-47a5-8c74-783352092891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
